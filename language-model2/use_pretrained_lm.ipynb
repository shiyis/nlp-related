{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-trained Language Model\n",
    "\n",
    "A statistical language model is simply a probability distribution over sequences of words or characters [1].\n",
    "In this tutorial, we'll restrict our attention to word-based language models.\n",
    "Given a reliable language model, we can answer questions like *which among the following strings are we more likely to encounter?*\n",
    "\n",
    "1. 'On Monday, Mr. Lamar’s “DAMN.” took home an even more elusive honor,\n",
    "one that may never have even seemed within reach: the Pulitzer Prize\"\n",
    "1. \"Frog zealot flagged xylophone the bean wallaby anaphylaxis extraneous\n",
    "porpoise into deleterious carrot banana apricot.\"\n",
    "\n",
    "Even if we've never seen either of these sentences in our entire lives, and even though no rapper has previously been\n",
    "awarded a Pulitzer Prize, we wouldn't be shocked to see the first sentence in the New York Times.\n",
    "By comparison, we can all agree that the second sentence, consisting of incoherent babble, is comparatively unlikely.\n",
    "A statistical language model can assign precise probabilities to each of these and other strings of words.\n",
    "\n",
    "Given a large corpus of text, we can estimate (or, in this case, train) a language model $\\hat{p}(x_1, ..., x_n)$.\n",
    "And given such a model, we can sample strings $\\mathbf{x} \\sim \\hat{p}(x_1, ..., x_n)$, generating new strings according to their estimated probability.\n",
    "Among other useful applications, we can use language models to score candidate transcriptions from speech recognition models, given a preference to sentences that seem more probable (at the expense of those deemed anomalous).\n",
    "\n",
    "These days recurrent neural networks (RNNs) are the preferred method for language models. In this notebook, we will go through an example of using GluonNLP to\n",
    "\n",
    "(i) implement a typical LSTM language model architecture\n",
    "(ii) train the language model on a corpus of real data\n",
    "(iii) bring in your own dataset for training\n",
    "(iv) grab off-the-shelf pre-trained state-of-the-art language models (i.e., AWD language model) using GluonNLP.\n",
    "\n",
    "## What is a language model (LM)?\n",
    "\n",
    "The standard approach to language modeling consists of training a model that given a trailing window of text, predicts the next word in the sequence.\n",
    "When we train the model we feed in the inputs $x_1, x_2, ...$ and try at each time step to predict the corresponding next word $x_2, ..., x_{n+1}$.\n",
    "To generate text from a language model, we can iteratively predict the next word, and then feed this word as an input to the model at the subsequent time step. The image included below demonstrates this idea.\n",
    "\n",
    "<img src=\"https://gluon.mxnet.io/_images/recurrent-lm.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "## Using a pre-trained AWD LSTM language model\n",
    "\n",
    "AWD LSTM language model is the state-of-the-art RNN language model [1]. The main technique leveraged is to add weight-dropout on the recurrent hidden to hidden matrices to prevent overfitting on the recurrent connections.\n",
    "\n",
    "### Load the vocabulary and the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/.mxnet/datasets/wikitext-2/wikitext-2-v1.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/wikitext-2/wikitext-2-v1.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /root/.mxnet/models/553458622608727088/553458622608727088_wikitext-2-be36dc52.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/wikitext-2-be36dc52.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /root/.mxnet/models/awd_lstm_lm_1150_wikitext-2-f9562ed0.zip56e16396-ca17-4b24-aa3f-7aa58f22978a from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/awd_lstm_lm_1150_wikitext-2-f9562ed0.zip...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWDRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 400, float32)\n",
      "    (1): Dropout(p = 0.65, axes=(0,))\n",
      "  )\n",
      "  (encoder): HybridSequential(\n",
      "    (0): LSTM(400 -> 1150, TNC)\n",
      "    (1): LSTM(1150 -> 1150, TNC)\n",
      "    (2): LSTM(1150 -> 400, TNC)\n",
      "  )\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(400 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp.utils.check_version('0.7.0')\n",
    "\n",
    "num_gpus = 1\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]\n",
    "log_interval = 200\n",
    "\n",
    "batch_size = 20 * len(context)\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.25\n",
    "\n",
    "dataset_name = 'wikitext-2'\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset, val_dataset, test_dataset = [\n",
    "    nlp.data.WikiText2(\n",
    "        segment=segment, bos=None, eos='<eos>', skip_empty=False)\n",
    "    for segment in ['train', 'val', 'test']\n",
    "]\n",
    "\n",
    "vocab = nlp.Vocab(\n",
    "    nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "\n",
    "\n",
    "# Batchify for BPTT\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "    vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = [\n",
    "    bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset]\n",
    "]\n",
    "\n",
    "awd_model_name = 'awd_lstm_lm_1150'\n",
    "awd_model, vocab = nlp.model.get_model(\n",
    "    awd_model_name,\n",
    "    vocab=vocab,\n",
    "    dataset_name=dataset_name,\n",
    "    pretrained=True,\n",
    "    ctx=context[0])\n",
    "\n",
    "print(awd_model)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the pre-trained model on the validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss 4.23, val ppl 68.80\n",
      "Best test loss 4.19, test ppl 65.73\n"
     ]
    }
   ],
   "source": [
    "# Specify the loss function, in this case, cross-entropy with softmax.\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "\n",
    "# Note that ctx is short for context\n",
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal\n",
    "\n",
    "\n",
    "val_L = evaluate(awd_model, val_data, batch_size, context[0])\n",
    "test_L = evaluate(awd_model, test_data, batch_size, context[0])\n",
    "\n",
    "print('Best validation loss %.2f, val ppl %.2f' % (val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f' % (test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a cache LSTM LM\n",
    "\n",
    "Cache LSTM language model [2] adds a cache-like memory to neural network language models. It can be used in conjunction with the aforementioned AWD LSTM language model or other LSTM models.\n",
    "It exploits the hidden outputs to define a probability distribution over the words in the cache.\n",
    "It generates  state-of-the-art results at inference time.\n",
    "\n",
    "<img src=cache_model.png width=\"500\">\n",
    "\n",
    "### Load the pre-trained model and define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CacheCell(\n",
      "  (lm_model): AWDRNN(\n",
      "    (embedding): HybridSequential(\n",
      "      (0): Embedding(33278 -> 400, float32)\n",
      "      (1): Dropout(p = 0.65, axes=(0,))\n",
      "    )\n",
      "    (encoder): HybridSequential(\n",
      "      (0): LSTM(400 -> 1150, TNC)\n",
      "      (1): LSTM(1150 -> 1150, TNC)\n",
      "      (2): LSTM(1150 -> 400, TNC)\n",
      "    )\n",
      "    (decoder): HybridSequential(\n",
      "      (0): Dense(400 -> 33278, linear)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "window = 2\n",
    "theta = 0.662\n",
    "lambdas = 0.1279\n",
    "bptt = 2000\n",
    "cache_model = nlp.model.train.get_cache_model(name=awd_model_name,\n",
    "                                             dataset_name=dataset_name,\n",
    "                                             window=window,\n",
    "                                             theta=theta,\n",
    "                                             lambdas=lambdas,\n",
    "                                             ctx=context[0])\n",
    "\n",
    "print(cache_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define specific get_batch and evaluation helper functions for the cache model\n",
    "\n",
    "Note that these helper functions are very similar to the ones we defined above, but are slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_batch_size = 1\n",
    "val_test_batchify = nlp.data.batchify.CorpusBatchify(vocab, val_test_batch_size)\n",
    "val_data = val_test_batchify(val_dataset)\n",
    "test_data = val_test_batchify(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data_source, i, seq_len=None):\n",
    "    seq_len = min(seq_len if seq_len else bptt, len(data_source) - 1 - i)\n",
    "    data = data_source[i:i + seq_len]\n",
    "    target = data_source[i + 1:i + 1 + seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cache(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    next_word_history = None\n",
    "    cache_history = None\n",
    "    for i in range(0, len(data_source) - 1, bptt):\n",
    "        if i > 0:\n",
    "            print('Batch %d, ppl %f' % (i, math.exp(total_L / i)))\n",
    "        if i == bptt:\n",
    "            return total_L / i\n",
    "        data, target = get_batch(data_source, i)\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        L = 0\n",
    "        outs, next_word_history, cache_history, hidden = model(\n",
    "            data, target, next_word_history, cache_history, hidden)\n",
    "        for out in outs:\n",
    "            L += (-mx.nd.log(out)).asscalar()\n",
    "        total_L += L / data.shape[1]\n",
    "        hidden = detach(hidden)\n",
    "    return total_L / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the pre-trained model on the validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000, ppl 60.767821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000, ppl 67.390511\n",
      "Best validation loss 4.11, val ppl 60.77\n",
      "Best test loss 4.21, test ppl 67.39\n"
     ]
    }
   ],
   "source": [
    "val_L = evaluate_cache(cache_model, val_data, val_test_batch_size, context[0])\n",
    "test_L = evaluate_cache(cache_model, test_data, val_test_batch_size, context[0])\n",
    "\n",
    "print('Best validation loss %.2f, val ppl %.2f'%(val_L, math.exp(val_L)))\n",
    "print('Best test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Merity, S., et al. “Regularizing and optimizing LSTM language models”. ICLR 2018\n",
    "\n",
    "[2] Grave, E., et al. “Improving neural language models with a continuous cache”. ICLR 2017"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}